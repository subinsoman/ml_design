version: "3.3"

services:
  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    restart: unless-stopped
    environment:
      OAUTH2_PROXY_PROVIDER: keycloak-oidc
      OAUTH2_PROXY_CLIENT_ID: sixdee
      OAUTH2_PROXY_CLIENT_SECRET: Y1Fe2ZIPX7hwB0SDIJNDjIOqj3HoDLN6
      OAUTH2_PROXY_OIDC_ISSUER_URL: http://sts-421:8080/auth/realms/sixdee
      OAUTH2_PROXY_REDIRECT_URL: http://sts-421:8080/oauth2/callback
      OAUTH2_PROXY_LOGIN_URL: http://sts-421:8080/auth/realms/sixdee/protocol/openid-connect/auth
      OAUTH2_PROXY_REDEEM_URL: http://sts-421:8080/auth/realms/sixdee/protocol/openid-connect/token
      OAUTH2_PROXY_PROFILE_URL: http://sts-421:8080/auth/realms/sixdee/protocol/openid-connect/userinfo
      OAUTH2_PROXY_LOGOUT_URL: "http://sts-421:8080/auth/realms/sixdee/protocol/openid-connect/logout?post_logout_redirect_uri=http://6dmagik:8080"
      OAUTH2_PROXY_COOKIE_SECRET: JFcMwnf6TLflowoOumQ2RXP8tOXcwAd5II_WjhqE6Ss=
      OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL: "true"
      OAUTH2_PROXY_EMAIL_DOMAINS: "*"
      OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
      OAUTH2_PROXY_COOKIE_SECURE: "false"
      OAUTH2_PROXY_COOKIE_HTTPONLY: "true"
      OAUTH2_PROXY_COOKIE_NAME: "MAGIK_SESSION"
      OAUTH2_PROXY_COOKIE_SAMESITE: "lax"
      OAUTH2_PROXY_COOKIE_DOMAINS: "sts-421"
      OAUTH2_PROXY_COOKIE_PATH: "/"
      OAUTH2_PROXY_COOKIE_REFRESH: "1h"
      OAUTH2_PROXY_COOKIE_EXPIRE: "0s"
      OAUTH2_PROXY_PASS_ACCESS_TOKEN: "true"
      OAUTH2_PROXY_PASS_ID_TOKEN: "true"
      OAUTH2_PROXY_SCOPE: "openid profile email"
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      OAUTH2_PROXY_CODE_CHALLENGE_METHOD: S256
      OAUTH2_PROXY_UPSTREAMS: "http://seahorse:33321"
      OAUTH2_PROXY_PASS_AUTHORIZATION_HEADER: "true"
      OAUTH2_PROXY_LOG_LEVEL: debug
      OAUTH2_PROXY_POST_LOGOUT_REDIRECT_URI: "http://sts-421:8080/"  # Final redirect after full logout
      OAUTH2_PROXY_BACKEND_LOGOUT_URL: "http://keycloak:8080/auth/realms/sixdee/protocol/openid-connect/logout?id_token_hint={id_token}&post_logout_redirect_uri=http://6dmagik:8080/logout-success"  # Dynamic provider logout with id_token
      OAUTH2_PROXY_WHITELIST_DOMAINS: "sts-421:8080"
    depends_on:
      keycloak:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 512M
    networks:
      default:
        ipv4_address: 192.168.0.41

  keycloak:
    image: quay.io/keycloak/keycloak:25.0.2
    restart: unless-stopped
    command: start-dev --hostname=sts-421 --http-relative-path=/auth  
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_HEALTH_ENABLED: "true"
      KC_HTTP_ENABLED: true
      KC_HOSTNAME: sts-421
      KC_HOSTNAME_PORT: 8080
      KC_HTTP_RELATIVE_PATH: /auth
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HOSTNAME_STRICT: false
      KC_PROXY_HEADERS: forwarded
      KC_PROXY_ADDRESS_FORWARDING: true
      KC_DB: mysql
      KC_DB_URL: jdbc:mysql://10.0.16.171:3306/mlops_keycloak
      KC_DB_USERNAME: subin
      KC_DB_PASSWORD: subin_123
      KC_FEATURES: hostname:v1
    healthcheck:
      test: ["CMD", "/opt/keycloak/bin/kc.sh", "show-config"]
      interval: 30s
      timeout: 10s
      retries: 10    
    deploy:
      resources:
        limits:
          memory: 512M
    networks:
      default:
        ipv4_address: 192.168.0.42
        aliases:
          - sts-421        

  code:
    image: codercom/code-server:latest
    restart: unless-stopped

    volumes:
      - ./project:/home/coder/project/sample   # Mount local project folder
      - ./code_data:/home/coder/.local/share/code-server  # Persist extensions/settings
    command: ["code-server", "--bind-addr", "0.0.0.0:8080", "--auth", "none"]
    networks:
      default:
        ipv4_address: 192.168.0.20
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001" --address ":9000"
    
    environment:
      MINIO_ROOT_USER: ""
      MINIO_ROOT_PASSWORD: ""
      # Use 127.0.0.1 to match redirect URLs
      MINIO_BROWSER_REDIRECT_URL: http://127.0.0.1:8080/minio
      
    volumes:
      - ./minio_data:/data
    networks:
      default:
        ipv4_address: 192.168.0.21
  # MLflow server
  mlflow:
    image: parthipa76/ml_flow-mlflow:v1
    ports:
      - "5000:5000"
    environment:
      MLFLOW_ARTIFACT_ROOT: s3://mlflow-artifacts
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: us-east-1
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command: >
      mlflow server
        --backend-store-uri mysql+pymysql://subin:subin_123@10.0.16.171:3306/mlflow_db
        --default-artifact-root s3://mlflow-artifacts
        --host 0.0.0.0
        --port 5000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 10s
      timeout: 5s
      retries: 12
    networks:
      default:
        ipv4_address: 192.168.0.22

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    ports:
      - "8080:8080"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx_logs:/var/log/nginx
      - ./nginx_includes:/etc/nginx/includes
    depends_on:
      keycloak:
        condition: service_healthy 
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      default:
        ipv4_address: 192.168.0.23
  database:
    image: quay.io/sixdee/seahorse-h2:alpine-jdk-14.0.2_12
    ports: []
    networks:
      default:
        ipv4_address: 192.168.0.13
    restart: always
    volumes:
      - ./seahorse_h2-data:/opt/h2-data:rw
  datasourcemanager:
    image: quay.io/subin/seahorse-datasourcemanager:5.0.1
    depends_on:
      - database
    environment:
      JDBC_URL: jdbc:h2:tcp://database:1521/datasourcemanager;DATABASE_TO_UPPER=false;DB_CLOSE_DELAY=-1
    links:
      - database
    networks:
      default:
        ipv4_address: 192.168.0.14
    ports: []
    restart: always
  documentation:
    image: quay.io/deepsense_io/seahorse-documentation:1.4.3
    networks:
      default:
        ipv4_address: 192.168.0.5
    ports: []
    restart: always
   
  frontend:
    depends_on:
      - documentation
      - workflowmanager
      - sessionmanager
      - library
      - notebooks
      - rabbitmq
    environment:
      API_VERSION: 1.4.3
      MQ_PASS: 1ElYfGNW
      MQ_USER: yNNp7VJS
      PORT: 80
      SESSION_POLLING_INTERVAL: 10000
    image: quay.io/sixdee/seahorse-frontend-cookie-removed:2020.03.25.02
    links:
      - documentation
      - workflowmanager
      - library
      - notebooks
      - rabbitmq
    networks:
      default:
        ipv4_address: 192.168.0.7
    ports: []
    restart: always
   
  library:
    image: quay.io/subin/seahorse-libraryservice:5.0.1
    networks:
      default:
        ipv4_address: 192.168.0.8
    ports: []
    restart: always
    volumes:
      - ./seahorse_library:/library
    
  mail:
    image: quay.io/deepsense_io/seahorse-mail:1.4.3
    networks:
      default:
        ipv4_address: 192.168.0.4
    ports: []
    restart: always
    
  notebooks:
    depends_on:
      - rabbitmq
      - workflowmanager
    environment:
      HEARTBEAT_INTERVAL: 2.0
      JUPYTER_LISTENING_IP: 0.0.0.0
      JUPYTER_LISTENING_PORT: 8888
      MISSED_HEARTBEAT_LIMIT: 30
      MQ_HOST: rabbitmq
      MQ_PASS: 1ElYfGNW
      MQ_PORT: 5672
      MQ_USER: yNNp7VJS
      WM_AUTH_PASS: 8Ep9GqRr
      WM_AUTH_USER: oJkTZ8BV
      WM_URL: http://workflowmanager:60103
    image: quay.io/deepsense_io/seahorse-notebooks:1.4.3
    #image: seahorse-notebooks:6b095dbd74fb9010953c2f4219d8e438336eae81
    links:
      - rabbitmq
      - workflowmanager
    networks:
      default:
        ipv4_address: 192.168.0.11
    ports: []
    restart: always
    volumes:
      - ./seahorse_library:/library
      - ./seahorse_forwarding_kernel_logs:/home/jovyan/forwarding_kernel_logs
   
  proxy:
    image: quay.io/deepsense_io/seahorse-proxy:1.4.3
    depends_on:
      - workflowmanager
      - sessionmanager
      - datasourcemanager
      - schedulingmanager
      - library
      - notebooks
      - rabbitmq
      - frontend  
      - documentation
    environment:
      AUTHORIZATION_HOST: https://8c2c6ad0cae34d19a69b94323c721bf3.api.mockbin.io/
      DATASOURCE_MANAGER_HOST: http://datasourcemanager:8080
      DOCUMENTATION_HOST: http://documentation:80
      ENABLE_AUTHORIZATION: 'false'
      FRONTEND_HOST: http://frontend:80
      JUPYTER_HOST: http://notebooks:8888
      LIBRARY_HOST: http://library:9083
      PORT: 33321
      RABBITMQ_HOST: http://rabbitmq:15674
      SCHEDULING_MANAGER_HOST: http://schedulingmanager:60110
      SESSION_MANAGER_HOST: http://192.168.0.1:9082  # Changed from 10.0.16.171 to container name
      WM_AUTH_PASS: 8Ep9GqRr
      WM_AUTH_USER: oJkTZ8BV
      WORKFLOW_MANAGER_HOST: http://workflowmanager:60103
      ALLOWED_DOMAINS: '*'
      #HTTPS_ENABLED: 'true'
      #SSL_CRT_FILE: '/opt/docker/certs/apache-selfsigned.crt'
      #SSL_KEY_FILE: '/opt/docker/certs/apache-selfsigned.key'
    links:
      - workflowmanager
      - datasourcemanager
      - schedulingmanager
      - library
      - notebooks
      - rabbitmq
      - frontend
      - documentation
    networks:
      default:
        ipv4_address: 192.168.0.6
    ports:
      - "33321:33321"  # Removed host IP binding
    restart: always
    volumes:
      - ./certs:/opt/docker/certs
    
  rabbitmq:
    environment:
      RABBITMQ_PASS: 1ElYfGNW
      RABBITMQ_USER: yNNp7VJS
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: -rabbit frame_max 134217728 -rabbit max_message_size 134217728
    image: quay.io/deepsense_io/seahorse-rabbitmq:1.4.3
    networks:
      default:
        ipv4_address: 192.168.0.9
    restart: always
  schedulingmanager:
    depends_on:
      - database
      - sessionmanager
      - workflowmanager
      - mail
    environment:
      JDBC_URL: jdbc:h2:tcp://database:1521/schedulingmanager;DATABASE_TO_UPPER=false;DB_CLOSE_DELAY=-1
      MAIL_SERVER_HOST: mail
      MAIL_SERVER_PORT: 25
      PORT: 60110
      SEAHORSE_EXTERNAL_URL: http://localhost:33321/
      SM_URL: http://192.168.0.1:9082  # Changed from 10.0.16.171 to container name
      WM_AUTH_PASS: 8Ep9GqRr
      WM_AUTH_USER: oJkTZ8BV
      WM_URL: http://workflowmanager:60103
    image: quay.io/deepsense_io/seahorse-schedulingmanager:1.4.3
    links:
      - database
      - workflowmanager
      - mail
    networks:
      default:
        ipv4_address: 192.168.0.3
    ports: []
    restart: always
 
  sessionmanager:
    image: ashishtom6d/seahorse-sessionmanager:7.0.3_12122024
    depends_on:
      - rabbitmq
      - workflowmanager
      - library
      - database
    environment:
      DATASOURCE_SERVER_ADDRESS: http://192.168.0.14:8080/datasourcemanager/v1/
      JDBC_URL: jdbc:h2:tcp://192.168.0.13:1521/sessionmanager;DATABASE_TO_UPPER=false;DB_CLOSE_DELAY=-1
      MAIL_SERVER_HOST: mail
      MAIL_SERVER_PORT: 25
      MQ_HOST: 192.168.0.9
      MQ_PASS: 1ElYfGNW
      MQ_PORT: 5672
      MQ_USER: yNNp7VJS
      NOTEBOOK_SERVER_ADDRESS: http://192.168.0.11:8888
      SM_HOST: 192.168.0.1
      SM_PORT: 9082
      SX_PARAM_PYTHON_DRIVER_BINARY: /opt/conda/bin/python
      SX_PARAM_PYTHON_EXECUTOR_BINARY: python
      SX_PARAM_SESSION_EXECUTOR_DEPS_PATH: /opt/docker/we-deps.zip
      SX_PARAM_SESSION_EXECUTOR_PATH: /opt/docker/we.jar
      SX_PARAM_SPARK_APPLICATIONS_LOGS_DIR: /spark_applications_logs
      SX_PARAM_SPARK_RESOURCES_JARS: /resources/jars
      SX_PARAM_TEMP_DIR: /tmp/seahorse/download
      SX_PARAM_WM_ADDRESS: 192.168.0.12:60103  # Changed from IP to container name
      SX_PARAM_WM_AUTH_PASS: 8Ep9GqRr
      SX_PARAM_WM_AUTH_USER: oJkTZ8BV
      HADOOP_CONF_DIR: /resources/data
      PYTHON_PATH: /resources/python_lib
    ports: []
    network_mode: host
    restart: always
    volumes:
      - ./seahorse_data:/resources/data
      - ./seahorse_jars:/resources/jars
      - ./seahorse_R_Libs:/opt/R_Libs
      - ./seahorse_spark_applications_logs:/spark_applications_logs:rw
      - ./seahorse_library:/library
      - ./seahorse_python_libs:/resources/python_lib
   
  workflowmanager:
    depends_on:
      - database
      - datasourcemanager
    environment:
      DATASOURCE_SERVER_ADDRESS: http://datasourcemanager:8080/datasourcemanager/v1/
      JDBC_URL: jdbc:h2:tcp://database:1521/workflowmanager;DATABASE_TO_UPPER=false;DB_CLOSE_DELAY=-1
      WM_AUTH_PASS: 8Ep9GqRr
      WM_AUTH_USER: oJkTZ8BV
      WM_HOST: 0.0.0.0
      WM_PORT: 60103
    image: quay.io/deepsense_io/seahorse-workflowmanager:1.4.3
    links:
      - database
      - datasourcemanager
    networks:
      default:
        ipv4_address: 192.168.0.12
    ports: []
    restart: always
    volumes:
      - ./seahorse_jars:/resources/jars
 
  
  airflow-init:
    image: parthipa76/airflow-ml-latest:2.7.2
    user: "0:0"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://subin:subin_123@10.0.16.171:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: "G7v546f7T-jqQ4UKt5ioBT7OrlsfZzHWmpUKhW99f0s="
      AIRFLOW__WEBSERVER__SECRET_KEY: "JqY7FS6dT4tT-rToY3ZeUQ4jeOBtUv7hIsZ216vk8gd68fD8xD1hh17W6IVUA_vQ1Wa-SK6gYVgWiWvd4FbN-w"
      # Disable authentication
      AIRFLOW__WEBSERVER__AUTHENTICATE: "False"
      AIRFLOW__WEBSERVER__RBAC: "False"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.default"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./airflow/webserver_config.py:/opt/airflow/webserver_config.py
    command: >
      bash -c "airflow db init"
    networks:
      default:
        ipv4_address: 192.168.0.29
   

        
  airflow:
    image: parthipa76/airflow-ml-latest:2.7.2
    user: "0:0"
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://subin:subin_123@10.0.16.171:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: "G7v546f7T-jqQ4UKt5ioBT7OrlsfZzHWmpUKhW99f0s="
      AIRFLOW__WEBSERVER__SECRET_KEY: "JqY7FS6dT4tT-rToY3ZeUQ4jeOBtUv7hIsZ216vk8gd68fD8xD1hh17W6IVUA_vQ1Wa-SK6gYVgWiWvd4FbN-w"
      AIRFLOW__WEBSERVER__BASE_URL: "http://127.0.0.1:8080/airflow"
      AIRFLOW__WEBSERVER__WEB_SERVER_PATH: "/airflow"
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
      AIRFLOW__CORE__ENABLE_XCOM_PICKLING: "True"

      # Disable authentication - corrected settings
      AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.www.security.NoAuth"

      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_S3_ENDPOINT_URL: "http://mlflow:9000"
      AWS_ACCESS_KEY_ID: "minio"
      AWS_SECRET_ACCESS_KEY: "minio123"
      AWS_DEFAULT_REGION: "us-east-1"
      AWS_S3_ADDRESSING_STYLE: "path"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./airflow/webserver_config.py:/opt/airflow/webserver_config.py
    ports:
      - "8050:8080"
    depends_on:
      - airflow-init
    networks:
      default:
        ipv4_address: 192.168.0.30
    command: webserver
 
    
  scheduler:
    image: parthipa76/airflow-ml-latest:2.7.2
    user: "0:0"
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://subin:subin_123@10.0.16.171:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: "G7v546f7T-jqQ4UKt5ioBT7OrlsfZzHWmpUKhW99f0s="
      AIRFLOW__WEBSERVER__SECRET_KEY: "JqY7FS6dT4tT-rToY3ZeUQ4jeOBtUv7hIsZ216vk8gd68fD8xD1hh17W6IVUA_vQ1Wa-SK6gYVgWiWvd4FbN-w"
      AIRFLOW__WEBSERVER__BASE_URL: "http://0.0.0.0:8080/airflow"
      AIRFLOW__WEBSERVER__WEB_SERVER_PATH: "/airflow"
      
      # Disable authentication - same settings as webserver
      AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.www.security.NoAuth"
      
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_S3_ENDPOINT_URL: "http://mlflow:9000"
      AWS_ACCESS_KEY_ID: "minio"
      AWS_SECRET_ACCESS_KEY: "minio123"
      AWS_DEFAULT_REGION: "us-east-1"
      AWS_S3_ADDRESSING_STYLE: "path"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
    depends_on:
      - airflow-init  
    networks:
      default:
        ipv4_address: 192.168.0.31  
    command: scheduler
  registry:
    image: registry:2.8
    restart: unless-stopped
    ports:
      - "5076:5000"  # Keep direct access for Docker CLI
    volumes:
      - ./docker_registry_data:/var/lib/registry
      - ./docker_registry_config/config.yml:/etc/docker/registry/config.yml:ro
    networks:
      default:
        ipv4_address: 192.168.0.33  
      
  registry-ui:
    image: joxit/docker-registry-ui:2.5.7
    restart: unless-stopped
    # Don't expose port directly, let nginx handle it
    environment:
      SINGLE_REGISTRY: "true"
      REGISTRY_TITLE: "Sixdee Docker Registry"
      DELETE_IMAGES: "true"
      SHOW_CONTENT_DIGEST: "true"
      CATALOG_ELEMENTS_LIMIT: "1000"
      NGINX_PROXY_PASS_URL: "http://registry:5000"
      NGINX_LISTEN_PORT: "8080"
      NGINX_RESOLVER: "127.0.0.11"
      BASE_URL: "/registry-ui/"
    ports:
      - "8076:8080"  # Keep direct access for Docker CLI  
    depends_on:
      - registry
    networks:
      default:
        ipv4_address: 192.168.0.32
   
  prometheus:
    image: prom/prometheus:latest
    volumes:
      # make sure prometheus.yml is next to this compose file, or use absolute path
      - ./prom/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus_data:/prometheus     # optional persistence for TSDB
    ports:
      - "9090:9090"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        ipv4_address: 192.168.0.52  
   
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:9091/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        ipv4_address: 192.168.0.53  
    
  grafana:
    image: grafana/grafana:latest
    environment:
      # Root URL with context path (e.g., http://localhost:3000/grafana)
      - GF_SERVER_ROOT_URL=http://localhost:3000/grafana
      # Enable serving from sub path
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      # Admin credentials
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
      # Enable anonymous access (no login required)
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      # Hide login form
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    volumes:
      - ./grafana_data:/var/lib/grafana
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:3000/grafana/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - prometheus
    networks:
      default:
        ipv4_address: 192.168.0.54

networks:
  default:
    ipam:
      config:
        - gateway: 192.168.0.1
          subnet: 192.168.0.0/24
      driver: default

volumes:
  minio_data: